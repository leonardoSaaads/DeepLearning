{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HANDS-ON 4: Funções de Ativação**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções de ativação são extremamente importantes para as redes neurais. Essas funções são as responsáveis por dar uma ampla gama de possibilidades para as redes neurais. Existe uma classe quase infinita de funções de ativações, mas iremos nos ater a somente as mais famosas nesse período. Veja algumas dessas funções:\n",
    "\n",
    "![](../Imagens/Unidade%201/activation_functions.png)\n",
    "\n",
    "Link da Imagem: [Clique Aqui](https://www.researchgate.net/figure/Some-of-the-most-common-activation-functions-and-their-first-order-gradient-From-left-to_fig4_346898697)\n",
    "\n",
    "Se utilizarnos um modelo simples - como uma equação linear - ela é simples de resolver, mas limita a sua capacidade de resolver problemas complexos. O que garante uma abordagem mais geral à resolução de problemas é não linearidade das funções de ativação. Uma rede neural sem função de ativação é essencialmente apenas um modelo de regressão linear. A função de ativação faz a transformação não-linear nos dados de entrada, tornando-o capaz de aprender e executar tarefas mais complexas. \n",
    "\n",
    "![](../Imagens/Unidade%201/modelo_funcao_ativacao.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Onde $W$ são os inputs multiplicados pelos seus respectivos pesos\n",
    "* O Símbolo $Σ$ representa o somatório dos valores de $W$\n",
    "\n",
    "Podemos escrever esse diagrama matematicamente como sendo:\n",
    "\n",
    "$$ output = \\text{FuncaoAtivação} \\left( \\sum_{n=0}^N input \\cdot weight + bias \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Função de Etapa Binária (Binary Step Function)**\n",
    "\n",
    "Essa função pode ser considerada como uma das mais simples. Ela é baseada na ideia de limiar, isto é, o neurônio é ativado se o valor de entrada for maior que o limiar. Ela pode ser usada ao criar um classificador binário. Quando simplesmente precisamos dizer sim ou não para uma única classe, a função de etapa seria a melhor escolha, pois ativaria o neurônio ou deixaria zero. Veremos posteriormente sobre a retropropagação(backpropagation, em inglês), mas matenha em mente que a retropropagação depende do gradiente da função de etapa( relacionado com a derivada da função de ativação).\n",
    "\n",
    "$$ f(x) = \\left\\{\\begin{matrix} 0 & \\text{ se x < 0} \\\\  1 & \\text{ se x>= 0} \\end{matrix} \\right.$$\n",
    "\n",
    "Como a derivada da função ativação é zero, utilizar retropropagação para ajustar os pesos não é viável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_step_function(x):\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Função Linear (Linear Function)**\n",
    "\n",
    "A função linear é muito boa para resoler problemas de ordem linear. Além disso, essa ativação é muito famosa pois pode ser utilizada para determinar a regressão em alguns dados. A derivada de uma função linear é constante, isto é, não depende do valor de entrada x. Isso significa que toda vez que fazemos backpropagation, o gradiente seria o mesmo. \n",
    "\n",
    "$$ f(x) = a \\cdot x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_step_function(a, x):\n",
    "    return a*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sigmóide**\n",
    "\n",
    "Esta é uma função suave e é continuamente diferenciável. Por ser uma função diferenciável, ela pode ser utilizada para ajustar os pesos de uma rede neural através de backpropagation.Além disso, devemos notar que a função varia entre os valores 0, valores mais voltados à esquerda, e 1, valores mais voltados a direita. A função sigmoide é dada por:\n",
    "\n",
    "$$ f(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simoide\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tangente Hiperbólica**\n",
    "\n",
    "A função tanh é muito semelhante à função sigmóide. Na verdade, é apenas uma versão escalonada da função sigmóide.\n",
    "\n",
    "$$ f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ReLU**\n",
    "\n",
    "ReLU é a função de ativação mais amplamente utilizada ao projetar redes neurais atualmente. A principal vantagem de usar a função ReLU sobre outras funções de ativação é que ela não ativa todos os neurônios ao mesmo tempo. O que isto significa ? Se você olhar para a função ReLU e a entrada for negativa, ela será convertida em zero e o neurônio não será ativado. Isso significa que, ao mesmo tempo, apenas alguns neurônios são ativados, tornando a rede esparsa e eficiente e fácil para a computação.\n",
    "\n",
    "$$ f(x) = \\max(0, x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax**\n",
    "\n",
    "A função softmax transforma as saídas para cada classe para valores entre 0 e 1 e também divide pela soma das saídas. A função softmax também é um tipo de função sigmóide, mas é útil quando tentamos lidar com problemas de classificação.\n",
    "\n",
    "$$ f(x) = \\frac{e^{x_1} + e^{x_2} + ... + e^{x_n}}{e^{x_1} + e^{x_2} + ... + e^{x_n}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Referência Bibliográfica**\n",
    "\n",
    "1 - **Capítulo 8 – Função de Ativação.** Disponível em: <https://www.deeplearningbook.com.br/funcao-de-ativacao/#:~:text=A%20fun%C3%A7%C3%A3o%20de%20ativa%C3%A7%C3%A3o%20%C3%A9%20a%20transforma%C3%A7%C3%A3o%20n%C3%A3o%20linear%20que,simplesmente%20fazem%20uma%20transforma%C3%A7%C3%A3o%20linear.>. Acesso em 17 de maio de 2022."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83db9ccd60aaba7f36d82c7c8a804da99e0bc213c9e856efe6e157fa32ac376b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
