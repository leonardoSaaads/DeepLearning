{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HANDS-ON 1: História das Redes Neurais e Métodos de Aprendizagem Profunda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1949 - Donald Hebb**\n",
    "\n",
    "A história das redes neurais e aprendizagem profunda teve seu início intrinsicamente relacionado com os modelos de machine learning. A história remonta a criação de um artigo científico publicado em 1943, nomeado como \"The Organization of Behavior\", o neuropsicologo Donald Olding Hebb propós um modelo matemático para a tomada de decisão para a cognição humana. Esse modelo matemático explorava como as células cerebrais realizavam suas interações - excitação e comunicação entre células. Em suas palavras, Hebb escreveu:\n",
    "\n",
    "\"Quando uma célula repetidamente ajuda a disparar (informação com) outra, o axônio da primeira célula desenvolve mecanismos sinápticos (ou aumenta-os se já existirem) em contato com o soma da segunda célula.\"\n",
    "\n",
    "Em outras palavras, a relação entre dois neurônios se fortalece se os dois neurônios forem ativados ao mesmo tempo e enfraquece se forem ativados separadamente. A palavra “peso” é usada para descrever essas relações, e neurônios que tendem a ser positivos ou negativos são descritos como tendo pesos positivos fortes. Aqueles nós que tendem a ter pesos opostos desenvolvem fortes pesos negativos (por exemplo, 1×1=1, -1x-1=1, -1×1=-1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1950 - O Jogo de Arthur**\n",
    "\n",
    "Na década de 1950, Arthur Samuel, um desenvolvedor e cientista da computação estadunidense, desenvolveu um programa de computador para jogar damas. Como nessa época não havia muita memória computacional disponível, Samuel começou a desenvolver o que é chamado de poda Alpha-Beta. Ele desenvolveu um algoritmo que estipulava as chances de vitória de cada lado utilizando uma função para calcular as chances de vitória com base em seus próximos movimentos - utilizando a estretégia do algorítmo MinMax.\n",
    "\n",
    "Nesse sentido, Samuel conseguiu fazer com que uma série de macanismos tornassem o seu programa cada vez melhor. Seu programa gravou/lembrou todas as posições que já tinha visto e combinou isso com os valores da função de recompensa. Arthur Samuel surgiu com a frase “aprendizagem de máquina” em 1952.\n",
    "\n",
    "<img src=\"../Imagens/Unidade%201/Alpha_Beta%20pruning.png\" alt=\"Alpha-Beta\" width=\"40%\">\n",
    "\n",
    "Link para imagem: [Clique Aqui](https://i.stack.imgur.com/9qPX9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1957 - Perceptron**\n",
    "\n",
    "Em 1957, Frank Rosenblatt, trabalhando no Laboratório Aeronáutico Cornell, influenciado pelo trabalho de Donald Hebb do modelo cerebral e da Máquina que aprende de Arthur Samuel, criou o \"Perceptron\". Originalmente o Perceptron seria uma máquina da empresa IBM -  teve até mesmo o seu nome \"batizado\" como Mark 1 perceptron e seria utilizada para reconhecimento de imagens.\n",
    "\n",
    "Apesar de ser descrito como primeiro neuro-computador, o projeto não teve boa adesão e foi considerado como um fracasso por muitos anos devido ao não reconhecimento de padrões mais complexos. A ideia foi retomada algumas décadas depois.\n",
    "\n",
    "<img src=\"../Imagens/Unidade%201/Modelo_perceptron_(pythonmachinelearning).png\" alt=\"Perceptron\" width=\"40%\">\n",
    "\n",
    "Link para imagem: [Clique aqui](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1967 - Algoritmo do Vizinho Mais Próximo**\n",
    "\n",
    "Em 1967, houve um grande avanço, pois foi o ano na qual o algoritmo do vizinho mais próximo foi criado, dando início ao reconhecimento de padrões mais complexos. Esse algoritmo foi usado para mapear rotas e foi um dos primeiros algoritmos usados ​​para encontrar uma solução para o famoso problema do vendedor ambulante (TSP) de encontrar a rota mais eficiente.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/23/Nearestneighbor.gif\" alt=\"Nearest neighbor\" width=\"40%\">\n",
    "\n",
    "Link para GIF: [Clique aqui](https://en.wikipedia.org/wiki/Travelling_salesman_problem#/media/File:Nearestneighbor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Década de 70 - Retropropação e Rede Neurais Profundas**\n",
    "\n",
    "Na década de 1970, um outro grande passo foi dado: a criação da retropropagação. A retropropagação é o mecanismo pelo qual os componentes que influenciam a saída de um neurônio (bias, pesos, ativações) são ajustados iterativamente para reduzir a função de custo. Na arquitetura de uma rede neural, a entrada do neurônio, incluindo todas as conexões anteriores com os neurônios da camada anterior, determinam sua saída.\n",
    "\n",
    "Alem disso, redes neurais com várias camadas (chamadas de layers em inglês) foram implementadas. Foi natural notar que usar duas ou mais camadas no perceptron oferecia significativamente mais poder de processamento do que uma camada perceptron. Com o aparecimento de redes maiores, o termo **Deep Neural Networks** (Redes Neurais Profundas, em português) ganhou força.\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1925/1*2EZ29YswQ50ByfDDtSIHLQ.gif\" alt=\"Backpropagation\" width=\"40%\">\n",
    "\n",
    "Link da Imagem: [Clique Aqui](https://miro.medium.com/max/1925/1*2EZ29YswQ50ByfDDtSIHLQ.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Década de 90 - Utilização de Otimizadores**\n",
    "\n",
    "Com o avanço cada vez mais rápido da computação, a necessidade por mecanismos capazes de acelerar determinadas operações entrou em voga. Um novo termo, designado de \"Boosting\" (Impulsionadores/Aceleradores/Otimizadores, em Português), emergiu. Os algoritmos \"Boosting\" são usados ​​para reduzir o viés durante o aprendizado supervisionado e incluem algoritmos de ML que transformam fracos aprendizados em fortes. O conceito de boosting foi apresentado pela primeira vez em um artigo de 1990 intitulado “The Strength of Weak Learnability”, de Robert Schapire. Em outras palavras, os algoritmos boosting são capazes de pegar programas com pequena correlação (Um pouco acima da média) e , através de uma ´serie de mecanismos, aumentar a sua correlação e, consequentemente, sua acurácia.\n",
    "\n",
    "A diferença básica entre os vários tipos de algoritmos de Boosting é “a técnica” usada na ponderação dos pontos de dados de treinamento. **AdaBoost** é um algoritmo de aprendizado de máquina popular e historicamente significativo, sendo o primeiro algoritmo capaz de trabalhar com aprendizado fraco. Algoritmos mais recentes incluem BrownBoost, LPBoost, MadaBoost, TotalBoost, xgboost e LogitBoost. Um grande número de algoritmos de reforço funciona dentro da estrutura AnyBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Anos 2000 - Reconhecimento Facial e Reconhecimento da Fala.**\n",
    "\n",
    "Em 1997, foi criada a técnica memória de longo prazo (LSTM), um modelo de rede neural descrito por Jürgen Schmidhuber e que pode aprender tarefas que exigem memória de eventos que ocorreram muitas etapas discretas antes. Apesar do modelo já existir, o sistema foi implementado de fato em 2007, quando sistemas de reconhecimento de fala foram implementados.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*goJVQs-p9kgLODFNyhl9zA.gif\" alt=\"LSTM\" width=\"40%\">\n",
    "\n",
    "\n",
    "Link da Imagem: [Clique Aqui](https://miro.medium.com/max/1400/1*goJVQs-p9kgLODFNyhl9zA.gif)\n",
    "\n",
    "Outro grande avanço se deu com as descobertas que emergiram no evento Face Recognition Grand Challenge, de 2006, na qual os novos algoritmos eram dez vezes mais precisos do que os algoritmos de reconhecimento facial de 2002 e 100 vezes mais precisos do que os de 1995.\n",
    "\n",
    "Em 2012, o X-Lab do Google desenvolveu um algoritmo de Machine Learning que pode navegar e encontrar de forma autônoma vídeos contendo gatos. Em 2014, o Facebook desenvolveu o DeepFace, um algoritmo capaz de reconhecer ou verificar indivíduos em fotografias com a mesma precisão dos humanos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **De 2020 até 2050+: Presente e o Futuro**\n",
    "\n",
    "- Fraud detection (detecção de fraude)\n",
    "- Customer relationship management systems (CRM)\n",
    "- Computer vision (visão computacional)\n",
    "- Vocal AI (inteligência vocal)\n",
    "- Natural language processing (processamento de linguagem natural)\n",
    "- Data refining (refinamento de dados)\n",
    "- Autonomous vehicles (veículos autônomos)\n",
    "- Supercomputers (computadores superiores)\n",
    "- Investment modeling (modelagem de investimentos)\n",
    "- E-commerce (comércio eletrônico)\n",
    "- Emotional intelligence (inteligência emocional)\n",
    "- Entertainment (entretenimento)\n",
    "- Advertising (anúncios)\n",
    "- Manufacturing (fabricação)\n",
    "- Healthcare (saúde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RELATÓRIO 01:** Realize uma pesquisa sobre algum método computaional - como LSTM, Deep Neural Networks, Boosting, etc - e relate o que você descobriu. Após a sua pequisa, faça um levanatamento das principais empresas que usam o modelo. Apresente o que você descobriu e o que você achou útil. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Referências Bibliográficas:**\n",
    "\n",
    "**1 - A Brief History of Machine Learning. By Keith D. Foote on December 3, 2021**. Disponível em: <https://www.dataversity.net/a-brief-history-of-machine-learning/>. Acessado em 07/05/2022.\n",
    "\n",
    "**2 - A Data Scientist’s Guide to Gradient Descent and Backpropagation Algorithms. By Richmond Alake. Foote on Feb 09, 2022**. Disponível em: <https://developer.nvidia.com/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/>. Acessado em 07/05/2022.\n",
    "\n",
    "**3 - 15 Deep Learning Applications You Need to Know. By Mike Thomas and Brian Nordli. Foote on Updated: March 7, 2022.** Disponível em <https://builtin.com/artificial-intelligence/deep-learning-applications>. Acessado em 07/05/2022.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
