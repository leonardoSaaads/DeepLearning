{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HANDS-ON 3: Os Primeiros Neurônios e as Primeiras Camadas (Parte 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos no [HANDS-ON 3: Criando os Primeiros Neurônios (Parte 01)](../Unidade%201/Hands-On_03.ipynb) que um modelo simples de neurônios pode ser feito com a seguite arquitetura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.78  0.035 3.385]\n"
     ]
    }
   ],
   "source": [
    "#Importanto Bibliotecas\n",
    "import numpy as np\n",
    "\n",
    "# Definindo os valores de entrada\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "\n",
    "# Definindo os pesos\n",
    "weights = [[0.12, 0.18, -0.15, 1.1],  \n",
    "           [0.15, -0.91, 0.26, -0.15],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "# Definindo valores de bias (Constantes)\n",
    "biases = [1.0, 1.3, 1.5]\n",
    "\n",
    "# Computando os valores de saída\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "# Printando resultados\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contudo, quando estamos criando e normalizando os valores para que uma determinada rede neural opere, os valores geralmente são passado em **lotes (batches, em inglês)**. Nos exemplos que trabalhos anteriormente, os valores dos inputs estavam sendo fornecidos um por vez (um Array Unidimensional), mas e se os valores fossem fornecidos como uma série de inputs de diversas observações (matriz numpy), ao invés de apenas uma observação por vez? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantos valores alocados: 3, e como estão alocados: (3,)\n",
      "Quantos valores alocados: 9, e como estão alocados: (3, 3)\n",
      "Quantos valores alocados: 18, e como estão alocados: (2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Modelo de Array Unidimensional. (modelos que estávamos usando):\n",
    "array_1D = np.array([1, 2, 3])\n",
    "\n",
    "# Modelo de Array Bidimensional.\n",
    "array_2D = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9]])\n",
    "\n",
    "# Modelo de Array Tridimensional.\n",
    "array_3D = np.array([[[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9]],\n",
    "                     [[10, 11, 12],\n",
    "                     [13, 14, 15],\n",
    "                     [16, 17, 18]]])\n",
    "\n",
    "# Dados do array Unidimensional\n",
    "print(f'Quantos valores alocados: {array_1D.size}, e como estão alocados: {array_1D.shape}')\n",
    "# Dados do array Bidimensional\n",
    "print(f'Quantos valores alocados: {array_2D.size}, e como estão alocados: {array_2D.shape}')\n",
    "# Dados do array Tridimensional\n",
    "print(f'Quantos valores alocados: {array_3D.size}, e como estão alocados: {array_3D.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante entendermos esses conceitos pois eles estão diretamente relacionados com os lotes que a rede neural irá receber. Comparando-se uma rede neural que recebe apenas um vetor unidimensional por vez e outra que recebe uma matriz dados por vez, é natural percebermos que a que recebe mais dados por vez tende a chegar ao equilibrio mais rapidamente. Em outras palavras, mais dados serão processados em paralelo, fazendo com que a rede consiga ter maior velocidade na hora do processamento. Uma observação importante a ser feita é que aumentar os valores dos lotes não implica em maior acurácia. Geralmente, o tempo de processamento diminui, mas, em compensação, a acurácia é afetada e pode diminuir.\n",
    "\n",
    "Como Kevin Shen expõe em seu artigo intitulado em inglês [\"Effect of batch size on training dynamics\"](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e) (Efeito dos lotes em treinamento dinâmico, em português):\n",
    "\n",
    "```{bibliography}\n",
    "O tamanho do lote é um dos hiperparâmetros mais importantes para ajustar os sistemas modernos de aprendizado profundo. Os praticantes geralmente desejam usar um tamanho de lote maior para treinar seu modelo, pois permite acelerações computacionais do paralelismo das GPUs. No entanto, é sabido que um tamanho de lote muito grande levará a uma generalização ruim (embora atualmente não se saiba por que isso acontece). [...]\n",
    "```\n",
    "\n",
    "Veja abaixo como pode ser o efeito dos lotes de entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ativação da primeira \"visualização\": [-0.05 -0.12 -0.03  0.39]\n",
      "Ativação da segunda \"visualização\": [-0.16 -0.4   0.09 -0.26]\n",
      "Ativação da terceira \"visualização\": [-0.31 -0.28  0.21 -0.14]\n",
      "Ativação da quarta \"visualização\": [ 0.28 -0.11 -0.13  0.4 ]\n",
      "Ativação da quinta \"visualização\": [-0.48 -0.02  0.25  0.43]\n",
      "Ativação da sexta \"visualização\": [-0.31 -0.41  0.32 -0.38]\n",
      "Ativação da sétima \"visualização\": [-0.35 -0.48 -0.15 -0.09]\n",
      "Ativação da oitava \"visualização\": [0.22 0.25 0.42 0.15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lote de várias ativações. Antes usávamos um por vez, agora iremos usar 8 de uma vez.\n",
    "batch_inputs = np.array([[-0.05, -0.12, -0.03, 0.39],   # primeiros inputs\n",
    "                         [-0.16, -0.4, 0.09, -0.26],    # segundos input\n",
    "                         [-0.31, -0.28, 0.21, -0.14],   # terceiros input\n",
    "                         [0.28, -0.11, -0.13, 0.4],     # quartos input\n",
    "                         [-0.48, -0.02, 0.25, 0.43],    # quintos input\n",
    "                         [-0.31, -0.41, 0.32, -0.38],   # sextos input\n",
    "                         [-0.35, -0.48, -0.15, -0.09],  # sétimos input\n",
    "                         [0.22, 0.25, 0.42, 0.15]])     # oitavos input\n",
    "\n",
    "\n",
    "print(f'Ativação da primeira \"visualização\": {batch_inputs[0]}')\n",
    "print(f'Ativação da segunda \"visualização\": {batch_inputs[1]}')\n",
    "print(f'Ativação da terceira \"visualização\": {batch_inputs[2]}')\n",
    "print(f'Ativação da quarta \"visualização\": {batch_inputs[3]}')\n",
    "print(f'Ativação da quinta \"visualização\": {batch_inputs[4]}')\n",
    "print(f'Ativação da sexta \"visualização\": {batch_inputs[5]}')\n",
    "print(f'Ativação da sétima \"visualização\": {batch_inputs[6]}')\n",
    "print(f'Ativação da oitava \"visualização\": {batch_inputs[7]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, como não estávemos utilizando lotes, as operações podiam ser moldadas com o produto escalar, contudo, quando incrementamos com uma matriz que contém uma série de inputs (\"visualizações\"), temos de achar um mecanismo que possibilite a multiplicação entre as matrizes. Na rede neural abaixo, temos um lote (matriz) contendo 4 entradas diferentes (arrays de ativação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lote de 4 \"visualizaçõe\" para 3 neurônios.\n",
    "batch_inputs = np.array([[0.47, 0.42, 0.46],\n",
    "                         [0.12, -0.15, -0.01],\n",
    "                         [-0.08, -0.1, 0.14],\n",
    "                         [-0.16, 0.13, -0.38]])\n",
    "\n",
    "# Pesos para três neurônios.\n",
    "weights = np.array([[0.42, -0.22, 0.45],\n",
    "                    [-0.04, 0.03, -0.29],\n",
    "                    [0.2, 0.34, -0.4]])\n",
    "\n",
    "# Bias para três neurônios.\n",
    "bias = np.array([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **NÂO PODEMOS** realizar a operação abaixo:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} \\color{blue} 0.47 & \\color{red} 0.42 & \\color{green} 0.46 \\\\ \\color{blue} 0.12 & \\color{red} -0.15 & \\color{green} -0.01 \\\\ \\color{blue} -0.08 & \\color{red} -0.1 & \\color{green} 0.14 \\\\ \\color{blue} -0.16 & \\color{red} 0.13 & \\color{green} -0.38 \\end{bmatrix}}_{\\text{Matriz Inputs (4,3)}} \\cdot \\underbrace{\\begin{bmatrix} \\color{teal}0.42 & \\color{teal}-0.22 & \\color{teal}0.45 \\\\ \\color{olive}-0.04 & \\color{olive}0.03 & \\color{olive}-0.29 \\\\ \\color{violet}0.2 & \\color{violet}0.34 & \\color{violet}-0.4 \\end{bmatrix}}_{\\text{Matriz com pesos (3,3)}} $$\n",
    "\n",
    "As cores $\\color{blue}Azul$, $\\color{red}Vermelho$ e $\\color{green} Verde$ da primeira matriz representam os valores de ativações em cada input. A primeira linha que contém essas cores representa a primeira entrada (ou \"visualizações\") de inputs, a segunda linha representa a segunda entrada (ou \"visualizações\") de inputs, a terceira linha representa a terceira entrada de inputs e assim sussecivamente.\n",
    "\n",
    "As cores da segunda coluna representam os pesos de cada neurônio. A cor $\\color{teal}Verde-azulado$ são os pesos do primeiro neurônio. A cor $\\color{olive}Oliva$ são os pesos do segundo neurônio e a cor $\\color{violet}Violeta$ são os pesos do terceiro neurônio.  Se fizessemos a multplicação acima, haveria um erro de lógica - pois estariamos multiplicando os valores de cada input por pesos que não pertenceriam a um determinado neurônio.\n",
    "\n",
    "Outra forma de visualizarmos esse erro é fazendo um outro lote de 3 entradas para quatro neurônios. Note que a multiplicação de matrizes usual (linha da primeira matriz multiplicada pela coluna da segunda matriz) não é definida para esse caso:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} \\color{blue} 0.47 & \\color{red} 0.42 & \\color{green} 0.46 & -0.16\\\\ \\color{blue} 0.12 & \\color{red} -0.15 & \\color{green} -0.01 &  0.13 \\\\ \\color{blue} -0.08 & \\color{red} -0.1 & \\color{green} 0.14 & -0.38 \\end{bmatrix}}_{\\text{Matriz Inputs (3,4)}} \\cdot \\underbrace{\\begin{bmatrix} \\color{teal}0.42 & \\color{teal}-0.22 & \\color{teal}0.45 & \\color{teal}0.92 \\\\ \\color{olive}-0.04 & \\color{olive}0.03 & \\color{olive}-0.29 & \\color{olive}-0.99 \\\\ \\color{violet}0.2 & \\color{violet}0.34 & \\color{violet}-0.4  & \\color{violet} 0.25\\end{bmatrix}}_{\\text{Matriz com pesos (3,4)}} $$\n",
    "\n",
    "Para corrigirmos esse problema, **utilizaremos a transposta da matriz com pesos**. Note que, para o primeiro caso, a multiplicação fica bem definida:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} \\color{blue} 0.47 & \\color{red} 0.42 & \\color{green} 0.46 \\\\ \\color{blue} 0.12 & \\color{red} -0.15 & \\color{green} -0.01 \\\\ \\color{blue} -0.08 & \\color{red} -0.1 & \\color{green} 0.14 \\\\ \\color{blue} -0.16 & \\color{red} 0.13 & \\color{green} -0.38 \\end{bmatrix}}_{\\text{Matriz Inputs (4,3)}} \\cdot \\underbrace{\\begin{bmatrix} \\color{teal}0.42 & \\color{olive}-0.04 &  \\color{violet}0.2 \\\\ \\color{teal}-0.22 & \\color{olive}0.03 & \\color{violet}0.34 \\\\ \\color{teal}0.45 & \\color{olive}-0.29 & \\color{violet}-0.4 \\end{bmatrix}^{T}}_{\\text{Matriz com pesos (3,3)}} $$\n",
    "\n",
    "Para o segundo caso, ela também fica bem definida:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix} \\color{blue} 0.47 & \\color{red} 0.42 & \\color{green} 0.46 & -0.16\\\\ \\color{blue} 0.12 & \\color{red} -0.15 & \\color{green} -0.01 &  0.13 \\\\ \\color{blue} -0.08 & \\color{red} -0.1 & \\color{green} 0.14 & -0.38 \\end{bmatrix}}_{\\text{Matriz Inputs (3,4)}} \\cdot \\underbrace{\\begin{bmatrix} \\color{teal}0.42 & \\color{olive}-0.04 & \\color{violet}0.2 \\\\ \\color{teal}-0.22 & \\color{olive}0.03 & \\color{violet}0.34 \\\\ \\color{teal}0.45 & \\color{olive}-0.29 & \\color{violet}-0.4  \\\\ \\color{teal}0.92 & \\color{olive}-0.99 & \\color{violet} 0.25 \\end{bmatrix}^{T}}_{\\text{Matriz com pesos (3,4)}} $$\n",
    "\n",
    "Podemos realizar esse comando em Numpy com o seguinte código abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.412  0.0604 0.3528] -> saida da primeira ativação\n",
      "[0.1789 0.1936 0.277 ] -> saida da segunda ativação\n",
      "[0.1514 0.1596 0.194 ] -> saida da terceira ativação\n",
      "[-0.1668  0.3205  0.4642] -> saida da quarta ativação \n",
      "\n",
      "saida da matriz completa:\n",
      " [[ 0.412   0.0604  0.3528]\n",
      " [ 0.1789  0.1936  0.277 ]\n",
      " [ 0.1514  0.1596  0.194 ]\n",
      " [-0.1668  0.3205  0.4642]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Lote de 4 \"visualizaçõe\" para 3 neurônios.\n",
    "batch_inputs = np.array([[0.47, 0.42, 0.46],\n",
    "                         [0.12, -0.15, -0.01],\n",
    "                         [-0.08, -0.1, 0.14],\n",
    "                         [-0.16, 0.13, -0.38]])\n",
    "\n",
    "# Pesos para três neurônios.\n",
    "weights = np.array([[0.42, -0.22, 0.45],\n",
    "                    [-0.04, 0.03, -0.29],\n",
    "                    [0.2, 0.34, -0.4]])\n",
    "\n",
    "# Bias para três neurônios.\n",
    "bias = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Fanzendo a transposta dos pesos:\n",
    "weights_T = weights.transpose()\n",
    "\n",
    "# Computando a ativação\n",
    "layer_outputs = np.dot(batch_inputs, weights_T) + bias\n",
    "\n",
    "# Printando resultados\n",
    "print(f'{layer_outputs[0]} -> saida da primeira ativação')\n",
    "print(f'{layer_outputs[1]} -> saida da segunda ativação')\n",
    "print(f'{layer_outputs[2]} -> saida da terceira ativação')\n",
    "print(f'{layer_outputs[3]} -> saida da quarta ativação \\n')\n",
    "\n",
    "print(f'saida da matriz completa:\\n {layer_outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que estamos conseguindo realizar os procedimentos para a nossa primeira camada de neurônios, podemos expandir nossa rede neural e criarmos uma segunda camada que recebe as saídas da primeira camada. As redes neurais tornam-se profundas quando possuem ao menos duas camadas ocultas. No\n",
    "momento, temos apenas uma camada, que é efetivamente uma camada de saída. Normalmente os cientistas de dados adicionam uma série de camadas para extrair partes dos dados. Um exemplo abaixo:\n",
    "\n",
    "![](../Imagens/Unidade%201/mnist_4.png)\n",
    "\n",
    "Note que a primera camada faz uma \"classificação\" de padrões na imagem de input. A segunda camada, por sua vez, realiza outras operações de \"classificações\" de padrões nos dados da primeira camada. Com várias camadas, podemos detectar padrões nas imagens e realizar melhor predições - por isso são utilizadas redes neurais profundas.\n",
    "\n",
    "Vamos abaixo criar nossa primeira rede com duas camadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saida da primeira camada: \n",
      "[[-0.152  -0.0762  0.1892]\n",
      " [-0.3018  0.0014  0.0972]\n",
      " [-0.1569  0.1457  0.1178]]\n",
      "\n",
      "Saida da segunda camada: \n",
      "[[ 0.437258  0.236028 -0.147816]\n",
      " [ 0.416756  0.268092 -0.120398]\n",
      " [ 0.466756  0.317466 -0.132171]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Entrada de dados\n",
    "batch_inputs = np.array([[0.33, 0.33, 0.3, 0.4],\n",
    "                         [-0.18, 0.28, 0.36, 0.32],\n",
    "                         [-0.34, -0.15, 0.48, -0.12]])\n",
    "\n",
    "\n",
    "# Primeira camada de neurônios\n",
    "# Pesos - Transposos\n",
    "weights_layer_1 = np.array([[0.31, -0.31, -0.48, -0.27],\n",
    "                            [-0.15, -0.29, -0.33, -0.08],\n",
    "                            [0.16, -0.22, -0.33, 0.02]]).T\n",
    "# Bias\n",
    "bias_layer_1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Segunda camada de neurônios\n",
    "# Pesos - Transposos\n",
    "weights_layer_2 = np.array([[0.19, 0.15, 0.04],\n",
    "                            [-0.16, 0.46, 0.3],\n",
    "                            [-0.37, 0.22, 0.49]]).T\n",
    "# Bias\n",
    "bias_layer_2 = np.array([0.47, 0.19, -0.28])\n",
    "\n",
    "# Output da primeira camada\n",
    "output_layer_1 = np.dot(batch_inputs, weights_layer_1) + bias_layer_1\n",
    "\n",
    "# Output da segunda camada - note que ele utiliza como entrada a saida da primeira camada.\n",
    "output_layer_2 = np.dot(output_layer_1, weights_layer_2) + bias_layer_2\n",
    "\n",
    "print(f'Saida da primeira camada: \\n{output_layer_1}\\n')\n",
    "print(f'Saida da segunda camada: \\n{output_layer_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na prática, o que criamos pode ser visto como o seguinte diagrama:\n",
    "\n",
    "![](../Imagens/Unidade%201/modelo_rede_neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa parte, podemos finalizar o conteudo de **Os Primeiros Neurônios e as Primeiras Camadas**.\n",
    "\n",
    "**RELATÓRIO 03:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Referências Bibliográficas:**\n",
    "\n",
    "1 - **Effect of batch size on training dynamics, Kevin Shen**. Disponível em: <https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e>. Acessado em 11 de maio de 2022\n",
    "\n",
    "4 - **Neural Nwtworks from Scratch in Python, Harrison Kinsley & Daniel Kukiela.**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83db9ccd60aaba7f36d82c7c8a804da99e0bc213c9e856efe6e157fa32ac376b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
