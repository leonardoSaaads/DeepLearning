{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HANDS-ON 5: Criando um Modelo Simplificado de Camadas - Dense Layer**\n",
    "\n",
    "Até agora utilizamos o que é chamado de camadas totalmente conectadas. Em inglês, a terminologia para essas camadas na programação e em artigos é \"Dense\". Como demonstrado no site da biblioteca Keras:\n",
    "\n",
    "```\n",
    "Dense implementa a operação: output = activation(dot(input, kernel) + bias) onde ativação é a função de ativação por elemento passada como argumento de ativação, kernel é uma matriz de pesos criada pela camada e bias é um vetor de bias criado pela camada (aplicável apenas se use_bias for True). Estes são todos os atributos de Densos.\n",
    "```\n",
    "\n",
    "Iremos utilizar como base a seguinte arquitetura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criando uma vlasse de camadas de neruônios do tipo totalmente conectados \"Dense\".\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "    # inicializa os pesos e bias com valores aleatorios\n",
    "        pass\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "    # Calcula o valor de saida com base nos inputs, pesos e bias.\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos utilizar pesos aleatórios para cada camada. Isso ajuda na função passo - fazendo com que haja que uma convergencia para o modelo. Um problema muito famoso anteriormente surgiu quando todos os neurônios tinhamseus pesos inciados em zero. Acontecia que o modelo fazia exatamente os mesmos cálculos em todas as camadas e, consequentemente, não havia aprendizado - é devido a esse motivo que as redes neurais \"quebram a simetria\" de pesos e possuem uma capacidade de aprender. Também é possível utilizar pesos e bias especificos para cada camada, mas, nesse caso, o modelo já teria que ter treinando com dados anteriormente para que os valores de pesos e bias fossem estabelecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criando uma vlasse de camadas de neruônios do tipo totalmente conectados \"Dense\".\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.1\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fazendo uma breve demonstração da classe que estamos criando:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos da camada 1:\n",
      "[[-0.10932781 -0.11593294  0.03556925]\n",
      " [ 0.0187206  -0.21949212 -0.10157492]\n",
      " [-0.09322372 -0.1217249  -0.02458365]]\n",
      "\n",
      "Biases da camada 1:\n",
      "[[0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gerando uma camada de neurônios e observando seus pesos e biases.\n",
    "\n",
    "camada_demonstracao = Layer_Dense(3, 3)  # Uma cada de 3 entradas e 3 neurônios.\n",
    "print(f'Pesos da camada 1:\\n{camada_demonstracao.weights}\\n')\n",
    "print(f'Biases da camada 1:\\n{camada_demonstracao.biases}\\n')\n",
    "\n",
    "# OBS: Note que toda ve que iniciamos esse código, os pesos são gerados aleatoriamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora realizar um teste de entrada com inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3696385 ,  0.1468098 ,  0.40550314],\n",
       "       [-0.78380178,  0.30550421,  0.96411677]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1, 2, 3],\n",
    "          [4, 5, 6]]\n",
    "\n",
    "camada_01 = Layer_Dense(3, 3)\n",
    "camada_01.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classes de Ativações.**\n",
    "\n",
    "Como vimos no Hands-On passado, existem diversas formas de ativações. Iremos criar as classes para algumas funções de ativação e implementar elas em nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ativação ReLU \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Calcula o valor de saida\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Ativação Softmax\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Obtém probabilidades não normalizadas\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normaliza para cada amostra\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelo de camadas com funções de ativação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camada 1: \n",
      "[[ 0.04697085  0.03786358 -0.0541443  -0.05833209]\n",
      " [ 0.06390732  0.12632154 -0.05764834 -0.00689434]\n",
      " [ 0.12192485  0.11595497 -0.11152636 -0.24515042]]\n",
      "\n",
      "Camada 2: \n",
      "[[-0.00722061 -0.00298639  0.01114767 -0.00323407]\n",
      " [-0.01579991  0.00364779  0.03138375 -0.00473809]\n",
      " [-0.02015452 -0.00593046  0.03276727 -0.00847467]]\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "\n",
    "batch_inputs = [[rd.random() for i in range(4)],\n",
    "                [rd.random() for i in range(4)],\n",
    "                [rd.random() for i in range(4)]]\n",
    "\n",
    "\n",
    "camada_01 = Layer_Dense(4, 4)\n",
    "activation_01 = Activation_ReLU()\n",
    "\n",
    "camada_02 = Layer_Dense(4, 4)\n",
    "activation_02 = Activation_Softmax()\n",
    "\n",
    "camada_01.forward(batch_inputs)\n",
    "activation_01.forward(camada_01.output)\n",
    "camada_02.forward(activation_01.output)\n",
    "activation_02.forward(camada_02.output)\n",
    "\n",
    "print(f'Camada 1: \\n{camada_01.output}\\n')\n",
    "print(f'Camada 2: \\n{camada_02.output}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83db9ccd60aaba7f36d82c7c8a804da99e0bc213c9e856efe6e157fa32ac376b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
